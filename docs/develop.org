** Tasks 

*** Main research direction
**** Day-to-day
***** TODO rank articles on relevance to XAI, feasibility, presence of source code or pseudcode, etc.
****** methods with new architectures -> eg. SoPa, state-regularized RNNs, FA-RNNs -> more direct learning where interpretability is built into main model -> might make sense to take an existing framework and extend it to interesting tasks and probe this for the extent of explainability
****** methods for FSA/WFSA extraction -> eg. spectral learning, clustering -> less direct interpretability and more proof of performance needed -> need to show it is better than simple data learning, might be more costly
***** constraint: work with RNNs only
***** constraint: seq2cls tasks -> eg. NLU/semantic, paraphrase detection
***** constraint: base main ideas off peer-reviewed articles 

**** Big-picture
***** develop a feasible, practical and interesting research question, writing will be easy when everything else is well defined
***** think about what this research adds that is not present -> possible to look at next steps in existing articles or possibly extension to new sequence classification tasks
***** high-performance -> show it is better than basic learning method from data -> implies we would have to use some negative examples from the oracle as well
***** explainable -> could potentially expose global bias for ethical and adversarial problem detections
***** keep probing networks/tasks as a backup option

*** Admin    
***** keep good communication with supervisors -> every 3 weeks for Sharid and more regularly with Mathias 
**** General timeline
***** +Initial thesis document: 15.09.20+
***** Topic proposal draft: 06.11.20
***** Topic proposal final: 15.11.20
***** Topic registration: 01.02.20  
***** Manuscript submission: 18.03.20, try to extend if possible  

*** Manuscript-specifics
***** read more about turing machines and FSA/WFAs to get theoretical background
***** ann's historical literature find all -> especially focusing on how ANNs approximate symbolic representations which would motivate overall topic
***** convergence, universal approximation and generalization are satisfied by ANNs to a high degree, semantic relevance in the final model is not guaranteed and this needs to be an additional task that where symbolic frameworks are needed    
***** limit main experiments on sequence classification but mention transducer extension to seq2seq
***** if possible, bring in theoretical CS and mathematics into paper
      
** Brainstorming 

*** Neuro-symbolic paradigms
***** research questions:
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions:
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables
