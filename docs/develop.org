#+STARTUP: overview
#+OPTIONS: ^:nil
#+OPTIONS: p:t

** Tasks
*** Manuscript 
**** Results
     DEADLINE: <2021-04-08 Thu>
***** TODO RQ2: Evaluation of explanations by simplification 
****** add table with information on performances and distance metrics averaged over random seeds with plus-minus deviation format
****** show plot of how these trends work and that increasing the tau threshold brings models closer together
***** RQ3: Interesting and insightful explanations on FMTOD
****** show plot of neuron importance distributions -> consider removing inter-neuron importance with alpha levels since this might be irrelevant
****** show relevant plots of regular expressions with neurons to see what kinds of textual patterns are imporant for the SoPa++ model and its regex proxy
******* figures must be manually put together later directly in latex
******* label the states consistently with "q" instead of only numbers
******* legend in neurons can be shifted below, perhaps to have just one copy for all neurons shown
******* tweak relative importances between neurons as well, if this is of use otherwise drop it -> need to add color to legend or otherwise fixed size with color gradient
******* consider adding subscripts to states
***** *Important:* everything shown in the "Results" section should have been well-motivated and should answer all three research questions

**** Discussion
***** Performance
****** mention issue of being unsure whether other studies removed duplicates -> but in our case using the same test set seems to improve evaluation performance
****** make statement on the competitiveness of these results
***** Explainability
****** make statement that explanations by simplifications appears to effective on the unseen evaluation set with similar scores; also a relationship can be observed with respect to the tau threshold
****** discuss how transparent the regex proxy really is given how many regular expressions are picked up -> link to how rules-based models in Arrieta et al 2020 paper can also become black-boxes -> provide numbers of regular expressions that get captured and stored
****** add segment on how useful this might be to a target audience based on the three criteria, but acknowledge that this would need to be consulted with a target audience
******* describe how a basic human evaluation of explainability could be done
******* mention visualization resources needed for this and it would have to be done with a much simpler and smaller model
***** Interesting and insightful observations
****** make statement to answer research question
****** inductive biases might be possible to find by looking into regular expressions
****** mention distributed representations in neurons where there is really no clear neuron responsible for one-thing -> which is an impediment to explainability since attribution and causal links are difficult to identify
***** Other discussion points
****** discuss relationship between tau threshold and the softmax/binary distances
****** expound on trade-off between performance and transparency by looking at differently sized models -> and then also looking at other studies which used BERTesque models
****** use discussions section to bring about more nuanced points on results
**** Conclusions
***** Summarize everything in manuscript
***** Address research questions
**** Further work
***** Modeling
****** use multiple-threads for extracting regular expressions, or store them in a database with indexing for faster regex lookups
****** extend to a finite-state transducer for seq2seq tasks
***** Explainability generalization/evaluation
****** can map linear to decision tree to get clearer picture of possibilities -> would make model even more transparent by removing continuous features
****** use nearest-neighbours to expand adjacent tokens
****** semantic clustering of common patterns for increased generalization
****** this is subjective and a survey from the target audience would be good to have -> would require an interactive interface where we visualize explanations
****** UNK token handling workflow for regex model where UNK has to representation other than indirect wildcards
****** human intervention inside regex model to monitor/improve performance
***** Analysis
****** extension to more NLU data sets such as SNIPS, ATIS
****** analyzing whether patterns can help discover possible adversarial patterns or inductive biases
****** for the target audience of end-users -> how can a user make use of the regex model
****** visualize examples/classes where regex and neural model align and misalign, eg. with a confusion matrix

**** Post-paper iteration/formatting
***** Introduction
****** abstract and introduction should already mention key findings
****** update motivations from Arrieta et al. 2020 "What for" section
****** add links to chapters in thesis structure
***** Background concepts
****** think more about how to improve (W)FA definitions
****** think more about adding document scoring algorithm in SoPa or whether to just leave it
****** pad definition environments with more explanatory text for flow
****** add more information on page numbers and sections in all Arrieta et al. citations so these seem more differentiated
****** explain vanilla SoPa in a more prose format using a table to explain important features -> this table can then be compared directly with new SoPa++ features
****** mention how or why SoPa falls into RNN and CNN categories
****** mention early on that quantized NNs are useful for low-precision computing, but we use it for other reasons later on
***** Methodologies
****** describe meaning of SoPa++ in C-symbology
****** mention target audience of explainability somewhere
****** consider adding sample notation to equations of distance metrics -> could help with results
****** improve table of differences with shared columns for similarities, or something else
****** consider adding background section on NLU overall and tasks available
****** mention the purpose of the intent detection task briefly on a grander-scheme-of-things
****** mention briefly the purpose of the slot filling task 
****** perhaps provide example of how softmax is conducted over weights to make this clearer
****** make the RE lookup layer more concise
****** we can analyze these to see how generalized these are or whether there could be strong inductive bias
***** Terminologies and abbreviations
****** sort out all abbreviations and standardize formatting in terms of where they are first declared -> perhaps add a page number on list of abbreviations to indicate first declaration
****** consider adding abbreviations directly into definitions
****** consider respelling "preprocessing" as "pre-processing" if necessary, fine-tune their usage
****** ensure linear-chain and strict linear-chain are added before WFAs
****** fine-tune antecedent/proxy terminology and synchronize everywhere
****** fine-tune usage of string length and document length while consuming
****** fine-tune usage of document vs. input utterance
****** fine-tune usage of consume a token and not consume a symbol
****** fine-tune usage of patterns vs. substrings vs. WFAs
****** fine-tune string consumption vs. string matching
****** fine-tune WFA to mean either automata or automaton, make plural abbreviation WFAs clear
****** fine-tune usage of FA vs. NFA and make these consistent with abbreviations versus full
****** fine-tune the slot-filling terminology 
****** fine-tune terminology between intent detection and intent classification
****** fine-tune terminology between data set and partition
****** fine-tune token length, sentence length and utterance length
***** Bibliography
****** improve capitalization with braces in bibtex file
****** find alternative journal/conference citations for current arxiv papers
***** Manuscript admin
****** Text-related
******* ensure that areas between chapters-sections and/or sections-subsections are filled with explanatory text to provide a narrative -> use links to/from individual sections/chapters to string everything together -> no area between title and next sub-title or environment should be empty -> an example is adding text before WFA definitions
******* replace all epsilon words by the symbol where possible 
******* make number of decimal places consistent wherever they are used such as in tables with tau
******* add remaining features by referring to master template such as abstract (short summarized introduction), list of tables/figures/abbreviations, appendices, etc; see master document for examples
******* change to two sided format before printing, as this works well for binding/printing
******* EITHER quote + indent sentences directly taken from other studies with page and section OR paraphrase them and leave them in a definition environment
******* check that all borrowed figures have an explicit attribution such as "taken from paper et al (year)"
******* perform spell-check of all text
******* change red link color in table of contents and modify color of URLs
******* always mention "figure taken from study (year)" when using external figures
****** UP-related
******* 20-90 pages thesis length -> well-motivated yet succinct
******* date on bottom of manuscript should be date of submission before mailing to Potsdam
******* add student registration details to paper such as matriculation number and other details
******* update title page date to current submission date
******* take note of all other submission criteria such as statement of originality, German abstract, digital copy and others, see: https://www.uni-potsdam.de/en/studium/studying/organizing-your-exams/final-thesis

*** Programming
**** Dependencies, typing and testing
***** if using R, document R dependencies with ~sessionInfo()~
***** look into cases where List was replaced by Sequential and how this can be changed or understood to keep consistency (ie. keep everything to List with overloads)
**** Documentation and clean-code
***** update readme and usages with finalized antecedent and proxy terminologies 
***** upadte readme and usages with finalized STE/output neurons terminologies
***** find attributable naming standards for PDFs produced with timestamp, perhaps dump a json file
***** GPU/CPU runs not always reproducible depending on multi-threading, see: https://pytorch.org/docs/stable/notes/randomness.html#reproducibility
***** add a comment above each code chunk which explains inner mechanisms better
***** update metadata eg. with comprehensive python/shell help scripts, comments describing functionality and readme descriptions for git hooks
***** add pydocstrings to all functions and improve argparse documentation
***** add information on best model downloads and preparation -> add these to Google Drive later on
***** test out all shell-scripts and python code to make sure everything works the same after major renamings
***** test download and all other scripts to ensure they work
***** perform formatting on latex code
***** ensure all label names and figure names are consitent
***** perform spell-check on readme 

** Notes
*** Admin
**** Timeline
***** +Initial thesis document: *15.09.2020*+
***** +Topic proposal draft: *06.11.2020*+
***** +Topic proposal final: *15.11.2020*+
***** +Topic registration: *01.02.2021*+
***** Offical manuscript submission: *12.04.2021*

** Legacy
*** Interpretable RNN architectures
**** State-regularized-RNNs (SR-RNNs)
***** good: very powerful and easily interpretable architecture with extensions to NLP and CV
***** good: simple code which can probably be ported to PyTorch relatively quickly
***** good: contact made with author and could get advice for possible extensions
***** problematic: code is outdated and written in Theano, TensorFlow version likely to be out by end of year
***** problematic: DFA extraction from SR-RNNs is clear, but DPDA extraction/visualization from SR-LSTMs is not clear probably because of no analog for discrete stack symbols from continuous cell (memory) states
***** possible extensions: port state-regularized RNNs to PyTorch (might be simple since code-base is generally simple), final conversion to REs for interpretability, global explainability for natural language, adding different loss to ensure words cluster to same centroid as much as possible -> or construct large automata, perhaps pursue sentiment analysis from SR-RNNs perspective instead and derive DFAs to model these
**** Rational recurences (RRNNs)
***** good: code quality in PyTorch, succinct and short
***** good: heavy mathematical background which could lend to more interesting mathematical analyses
***** problematic: seemingly missing interpretability section in paper -> theoretical and mathematical, which is good for understanding
***** problematic: hard to draw exact connection to interpretability, might take too long to understand everything
**** Finite-automation-RNNs (FA-RNNs)
***** source code likely released by November, but still requires initial REs which may not be present -> might not be the best fit
***** FA-RNNs involving REs and substitutions could be useful extensions as finite state transducers for interpretable neural machine translation

*** Interpretable surrogate extraction
***** overall more costly and less chance of high performance       
***** FA/WFA extraction
****** spectral learning, clustering
****** less direct interpretability
****** more proof of performance needed -> need to show it is better than simple data learning

*** Neuro-symbolic paradigms
***** research questions
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables      
