#+STARTUP: overview
 
** Tasks
*** Research
**** Interpretable neural architectures
***** TODO explore below frameworks (by preference) and find most feasible one
      DEADLINE: <2020-10-26 Mon>
***** Rational recurences (RRNNs)
****** TODO read and consider writing to RRNN author on interpretability analyses
****** good: code quality in PyTorch, succinct and short
****** good: heavy mathematical background which could lend to more interesting mathematical analyses
****** problematic: seemingly missing interpretability section in paper
***** State-regularized-RNNs (SR-RNNs)
****** TODO read through again to form more concrete opinions
****** good: very powerful and easily interpretable architecture with extensions to NLP and CV
****** problematic: code is outdated and written in Theano, TensorFlow version likely to be out by end of year
****** possible extensions: port state-regularized RNNs to PyTorch with CUDA headers (might be simple since code-base is generally simple), final conversion to REs for interpretability, global explainability for natural language
***** Soft patterns (SoPa)
****** good: practical new architecture which maps to RNN-CNN mix via WFSAs
****** good: code quality in PyTorch, lengthy code
****** problematic: global explainability might be a far shot, occlusion is still used for documents
****** problematic: not clear how this could be linked to a final WFSA
****** possible extensions: improve on larger data, learnable word embeddings, sub-word pre-processing to leverage morphology, increase generalization with wildcards, improve interpretability via in-built method instead of occlusion in document analysis setting
***** *GIST:* likely higher performance due to direct inference and less costly

**** Data sets
***** TODO search for popular NLU datasets which have existing RNN models as (almost) SOTAs
      DEADLINE: <2020-10-28 Wed>
***** TODO read more into these tasks and find one that has potential for interpretability -> likely reduce task to binary case for easier processing (eg. entailment)
      DEADLINE: <2020-10-28 Wed>

**** Clean-code and documentation
***** TODO write proposal and manuscript with key research questions which can be answered
      DEADLINE: <2020-11-06 Fri>
***** start populating repository with hooks, data downloads, documentation and models
 
**** Constraints
***** work with RNNs only
***** seq2cls tasks -> eg. NLU/semantic, paraphrase detection
***** base main ideas off peer-reviewed articles 

**** High-level
***** *globally explainable* -> exposes inner mechanisms and global biases which could help for ethical and adversarial problem detections
***** *high-performance* -> competitive with similar non-explainable learnint techniques
***** *contributions* -> should add insights which are new and not commonly found in research so far

*** Admin
**** General timeline
***** +Initial thesis document: 15.09.20+
***** Topic proposal draft: 06.11.20
***** Topic proposal final: 15.11.20
***** Topic registration: 01.02.20  
***** Manuscript submission: 18.03.20, try to extend if possible  
***** *Note:* meeting every 3 weeks with Sharid and more regularly with Mathias 

**** Manuscript notes
***** FSA/WFSAs -> input theoretical CS, mathematics background to describe these
***** ann's historical literature -> describe how ANNs approximate symbolic representations
***** extension/recommendations -> transducer for seq2seq tasks

** Completed
***** DONE add org-mode hook to remove startup visibility headers in org-mode to markdown conversion
      CLOSED: [2020-10-22 Thu 13:28]
***** DONE Set up repo, manuscript and develop log
      CLOSED: [2020-10-22 Thu 12:36]
      
** Legacy
*** Finite-automation-RNNs -> interpretable neural architecture
***** source code likely released by November, but still requires initial REs which may not be present -> might not be the best fit
***** FA-RNNs involving REs and substitutions could be useful extensions as finite state transducers for interpretable neural machine translation

*** Interpretable surrogate extraction
***** overall more costly and less chance of high performance       
***** FSA/WFSA extraction
****** spectral learning, clustering
****** less direct interpretability
****** more proof of performance needed -> need to show it is better than simple data learning

*** Neuro-symbolic paradigms
***** research questions:
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions:
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables
