#+STARTUP: overview
#+OPTIONS: ^:nil
#+OPTIONS: p:t

** Tasks
*** Manuscript
**** TODO Clean up all notes and add new deadlines to segments
**** Methodologies
***** FMTOD data set
****** provide summary statistics and explanation about the data set
****** provide a visualization on the data set and its splits
****** mention that data set was altered to keep it in good quality by removing duplicates, perhaps this can be re-tested without such processing or used as a limitation/further-work -> although results appear to show that performance metric improve when using original data set because of duplicates and overlaps between train and test sets
****** talk about lowercasing, upsampling and other important information
***** Model
****** update the WFSA definitions to signify wildcard transitions and mention why this is found here and not in the background concepts since we could not find literature which defined it in a similar way
****** add pseudocode for various segments -> would help to cement certain concepts
****** look out for misconception between tau and transition matrix symbol 
****** try to find literature-based justification for wildcard transition -> if not use omega symbol to differentiate from the Kleene star symbol -> use UTF-8 symbol for graphviz plots
****** add detailed information on how hard SoPa++ model differs from SoPa related to transitions and other simplifications -> motivate them using idea of explainable simplification
****** comb through terms and iron out usage of patterns vs. substrings vs. WFSAs -> make these consistent and have them only refer to one consistent entity, also do not mix colloquial and technical terms
****** talk about GloVe embeddings, lowercasing and other important things
****** neural SoPa++ is a black-box (non-transparent) model, regex proxy is a transparent model -> need justifications from background concepts, might need significant text on this portion
****** SoPa++ uses explanation by simplification (globally) -> need justification from background concepts -> not much use of global in paper, but we can make our own arguments
****** try to link as much as possible with the background concepts for models/explainability concepts
****** add github link to repo as a footnote
***** Explainability
****** explain how we make explanations by simplification work altogether
****** utilize antecendent/proxy model terminologies
****** mention tangible metric for simplification -> how close is proxy to antecedent
****** untangible metric is how satisfying the proxy model is to a target audience -> future work
****** hard to find hierarchies of good vs. not-good explainability, but we can argue that we tried a different explainability method, i.e. explanation by simplification with a global simplified model vs. local-explanations/feature-relevance -> also we can use the constrictive argument from the paper and others
****** make claim that SoPa++ explainability has a different explainability taxonomy from that of vanilla SoPa, but don't claim it is decisively better
****** use the three good explainability criteria to show that our technique might be better theoretically, but the real test would have to be done with a target audience's survey
****** mention that the target audience of this explainability method is domain experts, since it is still very complicated
****** link back to background concepts when we discuss bringing neural and regex model as close to each other as possible
***** Quantization/Binarization
****** explain how and why we modified STE to TauSTE
****** how does binarizing help with explainability, justify requirement for it in both training and testing
***** Training/Evaluation/Explainability-evaluation
****** provide extensive details of training setup
****** provide extensive details of evaluating neural/regex models
****** provide extensive details of evaluating explanations by simplification "metric" of neural-regex models -> which should make results clearer
****** *important:* everything shown in the "Results" section should be motivated or introduced here
***** Visualizations
****** add visualization of TauSTE function
****** produce manual computational graph using tikz, building from what was made earlier

**** Results
***** Report F_1 scores and performances of neural models
****** modify visualize scripts to aggregate and print summary stats in script to re-use later in paper with means and standard deviations across random seeds
****** report parameter counts in scores as well
****** compare performance to that of other paper(s)
****** mention again about test partition difference due to making it unique
****** consider making test-partition not unique so this could be used to compare with other studies
***** Relationship between tau threshold vs. performance vs. softmax/binary distances 
****** compute statistics with random-seed deviations over inter-model comparisons such as average distance, misalignment, activation frequency and other useful metrics that can elucidate on-the-ground processes
****** go into details on how effective compression algorithm was in terms of reducing the memory and number of regex's -> can tabulate all of these
***** Visualizations
****** show visualization of training performance timelines, think about how to keep most important information
****** show visualization of tau threshold vs. performance vs. softmax/binary distances with error bars for random seed iterations
****** show confusion matrix between regex and neural models to show alignment/misalignment, if this is necessary -> but it will show interesting class differences

**** Discussion
***** Expound on trade-off between performance and transparency by looking at differently sized models
***** Discuss relationship between tau threshold and the softmax/binary distances
***** Think about why larger regex models tend to show more misalignment from neural counterparts compared to smaller models
***** If possible, add some minor human evaluation of explainability otherwise leave it to future work
***** Explain discussion figures very clearly and show the relevance to the third research question, can talk about neurons responsible for certain decisions, as well as distributed representations in neurons where there is really no clear neuron responsible for one-thing -> which is an impediment to explainability
***** Visualizations
****** show visualizations of important patterns in a regex model -> something which is small and fits well into a page
******* figures must be manually put together later directly in latex
****** show TikZ visualization of each binary neuron's relative importance for classes -> would be interesting to see how saturated these are 

**** Conclusions
***** Summarize everything in manuscript
***** Address research questions

**** Further work
***** Quality of explainability
****** this is subjective and a survey from the target audience would be good to have
***** Modeling
****** use packed sequences for fast processing
****** use multiple-threads for extracting regular expressions, or store them in a database
****** more thorough regex lookup since now only the first one causes a loop breakage
****** add predict function for people to test with arbitrary sequences
****** consider using nearest-neighbours to expand adjacent tokens on already found regex's
****** consider internal regex UNK token handling cases other than wildcard presence
****** consider semantic clustering of digits or other objects to help achieve generality
****** extend to a finite-state transducer for seq2seq tasks
****** can map linear to decision tree to get clearer picture of possibilities
****** human intervention inside regex model to monitor/improve performance
***** Analysis
****** extension to more NLU data sets such as SNIPS, ATIS
****** analyzing whether patterns can help discover possible adversarial patterns
****** for the target audience of end-users -> how can a user make use of the regex model
****** visualize examples/classes where regex and neural model align and misalign, eg. with a confusion matrix

**** Post-paper iteration/formatting
***** Paper length
****** 20-90 pages thesis length -> try to keep ideas well-motivated yet succinct
***** Points to address towards end
****** Introduction
******* abstract and introduction should already mention results, and should not leave this to conclusions
******* fine-tune introduction with new details from other chapters
******* update motivations from Arrieta et al. 2020 "What for" section
******* add C-like reference to explain what SoPa++ means like in i++
******* add links to chapters in thesis structure, improve formatting
****** Background concepts
******* explain vanilla SoPa more clearly to motivate everything else -> perhaps need more information on FSAs with starting and accepting states
******* EITHER quote + indent sentences directly taken from other studies (cite pages and paragraphs) OR paraphrase them and leave them in a definition environment
******* be very clear on what is directly taken from another study versus what is paraphrased
******* definition/remark structure might need to be revised to something more narrative-suited
******* think about providing an additional definition for "understandability" 
******* consider quoting all definitions to further imply that they are exactly taken from other studies
******* add a Kleene-star operator mention to remark 9.4
******* include a section on risks on large NLP models and why explainability is necessary with different study
******* if possible, try to reduce references to Arrieta et al. 2020 to reduce perceived over-dependence
******* revisit sopa explainability evaluation with three guidelines to check if it makes sense after having evaluated sopa++ with the same guidelines
******* look into antecedent/proxy names and if these can be improved
******* return to this chapter to add/remove content based on requirements of later chapters
****** Bibliography
******* change FMTOD citation to NAACL, look for journal/conference alternative citations for current papers
******* improve capitalization with braces in bibtex file
******* if possible, try to find non-arxiv citations for papers
******* remove red link color in table of contents
******* fine-tune citation color to be consistent with other colors
****** Manuscript admin
******* read manuscript and ensure there is an easily followable narrative for someone who is a non-expert
******* improve file structure of visuals with explicit folders for directly copied figures
******* add titles to all figures in the manuscript
******* consider replacing legacy-sopa figures with pdf extracts instead of screenshots
******* talk to supervisors about many definitions and if these are alright
******* always mention "figure taken from study (year)" when using external figures
******* fine tune WFSA to mean either automata or automaton, make plural abbreviation clear as well
******* remove sub-enumeration for single remarks under a definition 
******* add links to different sections later on once structure and content is clear
******* sort out all abbreviations and standardize formatting in terms of where they are first declared
******* change to two sided format before printing, as this works well for binding/printing
******* add Uni-Potsdam originality declaration, or modify current one to fit
******* add remaining features by referring to master template such as abstract (short summarized introduction), list of tables/figures/abbreviations, appendices, and all others
******* perform spell-check of everything at the end
       
*** Programming
**** Dependencies, typing and testing
***** if using R, document R dependencies with ~sessionInfo()~
***** add mypy as an explicit part of testing the source code
***** replace Union + None types with Optional type for conciseness
***** look into cases where List was replaced by Sequential and how this can be changed or understood to keep consistency (ie. keep everything to List with overloads)
***** include basic test code by instantiating class and/or other simple methods
**** Documentation and clean-code
***** Terminology-based modifications post-paper 
****** if necessary, apply further script renaming using antecedent and proxy terminologies -> update readme and usages
****** consider removing ~utils~ extension from all utils scripts since these might be unnecessary
****** test out all shell-scripts and python code to make sure everything works the same after major renamings
***** Others
****** find a better way of naming visualization pdfs to attribute to specific model
****** fix terminology of STE/output neurons consistently after paper
****** GPU/CPU runs not always reproducible depending on multi-threading, see: https://pytorch.org/docs/stable/notes/randomness.html#reproducibility
****** reduce source code lines, chunking and comments -> pretty sort python code and function/class orders perhaps by length
****** add a comment above each code chunk which explains inner mechanisms better
****** update metadata eg. with comprehensive python/shell help scripts, comments describing functionality and readme descriptions for git hooks
****** add information on best model downloads and preparation -> add these to Google Drive later on
****** add pydocstrings to all functions and improve argparse documentation
****** provide description of data structures (eg. data, labels) required for training processes and lowercasing
****** update/remove git hooks depending on which features are finally used, eg. remove pre-push hook
****** test download and all other scripts to ensure they work
****** perform spell-check on readme 
       
** Notes
*** Admin
**** Timeline
***** +Initial thesis document: *15.09.2020*+
***** +Topic proposal draft: *06.11.2020*+
***** +Topic proposal final: *15.11.2020*+
***** +Topic registration: *01.02.2021*+
***** Manuscript draft submission: *31.03.2021* 
***** Offical manuscript submission: *11.04.2021*

** Legacy
*** Interpretable RNN architectures
**** State-regularized-RNNs (SR-RNNs)
***** good: very powerful and easily interpretable architecture with extensions to NLP and CV
***** good: simple code which can probably be ported to PyTorch relatively quickly
***** good: contact made with author and could get advice for possible extensions
***** problematic: code is outdated and written in Theano, TensorFlow version likely to be out by end of year
***** problematic: DFA extraction from SR-RNNs is clear, but DPDA extraction/visualization from SR-LSTMs is not clear probably because of no analog for discrete stack symbols from continuous cell (memory) states
***** possible extensions: port state-regularized RNNs to PyTorch (might be simple since code-base is generally simple), final conversion to REs for interpretability, global explainability for natural language, adding different loss to ensure words cluster to same centroid as much as possible -> or construct large automata, perhaps pursue sentiment analysis from SR-RNNs perspective instead and derive DFAs to model these
**** Rational recurences (RRNNs)
***** good: code quality in PyTorch, succinct and short
***** good: heavy mathematical background which could lend to more interesting mathematical analyses
***** problematic: seemingly missing interpretability section in paper -> theoretical and mathematical, which is good for understanding
***** problematic: hard to draw exact connection to interpretability, might take too long to understand everything
**** Finite-automation-RNNs (FA-RNNs)
***** source code likely released by November, but still requires initial REs which may not be present -> might not be the best fit
***** FA-RNNs involving REs and substitutions could be useful extensions as finite state transducers for interpretable neural machine translation

*** Interpretable surrogate extraction
***** overall more costly and less chance of high performance       
***** FSA/WFSA extraction
****** spectral learning, clustering
****** less direct interpretability
****** more proof of performance needed -> need to show it is better than simple data learning

*** Neuro-symbolic paradigms
***** research questions
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables      
