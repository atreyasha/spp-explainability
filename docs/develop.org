** Tasks
   
*** Main research direction
**** Day-to-day
***** TODO delete old articles and keep relevant ones to focus on 
***** TODO find applications of FSA/WFAs in NLP -> decipher if they are generally performant and/or explainable
****** SoPa is a very close idea which links WFSAs and ANNs -> but this is proposed as a new architecture altogether -> this might be the best way to start off
****** alternatively, we can try to learn directly from trained models -> worth considering whether black-box or white-box techniques are better for this 
***** TODO look into algorithms to extract FSA/WFAs from RNNs
***** TODO if all works, finalize seq2cls tasks -> eg. verb-object agreement, NLU, paraphrase detection, or otherwise semantics-oriented tasks
***** TODO read more about turing machines and FSA/WFAs to get theoretical background
***** constraints: work with RNNs, focus on NLU/semantic seq2cls task
***** constraints: base main ideas off peer-reviewed articles 

**** Big-picture
***** think about what this research adds that is not present -> possible to look at next steps in existing articles or possibly extension to new sequence classification tasks
***** high-performance -> show it is better than basic learning method from data -> implies we would have to use some negative examples from the oracle as well
***** explainable -> could potentially expose global bias for ethical and adversarial problem detections
***** keep probing networks/tasks as a backup option

*** Admin    
***** keep good communication with supervisors -> every 3 weeks for Sharid and more regularly with Mathias 
**** General timeline
***** +Initial thesis document: 15.09.20+
***** Topic proposal draft: 06.11.20
***** Topic proposal final: 15.11.20
***** Topic registration: 01.02.20  
***** Manuscript submission: 18.03.20, try to extend if possible  

*** Manuscript-specifics
***** ANNs historical literature find all -> especially focusing on how ANNs approximate symbolic representations which would motivate overall topic
***** convergence, universal approximation and generalization are satisfied by ANNs to a high degree, semantic relevance in the final model is not guaranteed and this needs to be an additional task that where symbolic frameworks are needed    
***** limit main experiments on sequence classification but mention transducer extension to seq2seq
***** if possible, bring in theoretical CS and mathematics into paper
      
** Brainstorming 

*** Neuro-symbolic paradigms
***** research questions:
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions:
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables
