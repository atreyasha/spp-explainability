#+STARTUP: overview
#+OPTIONS: ^:nil
#+OPTIONS: p:t
  
** Tasks
*** Research
**** Clean code and documentation
***** Current
****** TODO major changes to architecture and workflow
       DEADLINE: <2020-12-24 Thu>
******* technique: execute code on low batch level with preset arg namespace in =soft_patterns.py= and =train.py= -> use this to understand batch level processes and address limitations below
******* core model developments
******** improve code quality with unique model logging and tensorboard workflows 
******** use separate tokenizers such as nltk or sentencepiece tokenizer from Transformers library 
******** why are =*START*= and =*END*= tokens repeated before and after, and why is =*UNK*= used for padding when a separate =*PAD*= token could be used?
********* review and opine whether this needs to be changed -> perhaps there is a reason but this needs to be taken into account in the interpretation/visualization phase
******** dynamic OR static word/sub-word embeddings
********* unknown, start and end GloVe vector should be learned, not set to zero
********* OR perhaps there is merit in keeping input vectors fixed to prevent overfitting in small subset of them
********* OR perhaps modify such that the =*UNK*= token can be used for learning over the dataset, while =*START*=, =*END*= and =*PAD*= tokens stay as zeroes to complement overall model
********* consider still using dynamic embeddings versus static -> can use a lower learning rate for embeddings to reduce overfitting as much as possible
********* sub-word non-contextual embeddings are available as well, see: fastText or https://nlp.h-its.org/bpemb/#cite
******** experiment more gracious self-loops and epsilon transitions for improved generalization 
******** modify final layer to a general additive layer with tree structure or soft logic where possible -> perform this incrementally for comparison
******** use data processor class from torch where possible and flexible -> not necessarily urgent but could be interesting 
******** make incremental tree of changes with grid-search and random-seed-variant repeats -> do grid search and multiple runs of each best model with different random seeds to get standard deviation of performance
******* core explainability changes
******** focus on explainability rather than performance since the data set is relatively easy to perform on -> this part should be well studied and motivated -> final ensemble of regular expressions should give insights and perform similar to main SoPa++ neural model
******** revert/refactor soft_patterns_rnn, visualization, interpretation and testing scripts from git backlog to repository -> understand and improve these significantly
******** best case scenario: user should be able to transfer easily between models and regex-ensemble in both directions for "human-computer interaction"
        
******* diverse minor issues
******** testing/typing
********* add mypy as a test case suite, design new and improved test cases using pytest after understanding code completely
********* address debug level issues throughout code -> esp. where it affects clear typing in =forward=
********* fine-tune typing in internal functions of =SoftPatternClassifier= since some of them require batch-level testing to ascertain, eg. =get_transition_matrices=, =load_pattern= -> need to ascertain wither =pre_computed_patterns= is List or List[List[str]] -> consider removing =float= from =self_loop_scale: Union[torch.Tensor, float, None]= in =transition_once=
********* read more about purpose of using mypy and workflow to integrate it into stack or testing -> good for overall testing
********* look into cases where List was replaced by Sequential and how this can be changed or understood to keep consistency (ie. keep everything to List)
******** argparse/logging/dependencies
********* replace input arg namespace with explicit arguments, OR possible to make separate argparse Namespace which can be passed to main, this could help with portability
********* change argument names later on as this might break things in actual scripts
********* add discrete choices in arg_parser for =--shared-sl=
********* fix up python logging verbosity and pass tqdm directly to logger instead of purely to stdout -> integrate this well with debug argument as per legacy sopa: see https://github.com/tqdm/tqdm/issues/313
********* use =renv= for managing and shipping R dependencies -> keep just =renv.lock= for easier shipping and ignore other files
******** miscellaneous
********* =torch.nn.Module=
********** creating tensors inside =torch.nn.Module= keeps them out of autograd, initializing other =torch.nn.Module= subclasses creates =Paramater= classes which learn by default, therefore it is necessary to instantiate learnable parameters in a custom =torch.nn.Module= with the =Paramter= class -> other tensors default to no learning
********** experiment module with and without =fixed_var= to see if =autograd= gets enabled
********** need to understand =nn.Module= functionality before anything else -> investigate whether =fixed_var= function is indeed necessary or can be removed since =requires_grad= is set to False by default, but could be some conflict with =nn.Module= default parameter construction with ~requires_grad = True~ -> left intact for now and appears to work well
********* look into replacing =loss.data= with =loss.detach()=: see https://pytorch.org/blog/pytorch-0_4_0-migration-guide/#what-about-data
********* look into iteration progress snippet with gold and predicted "1's" meaning
********* confirm whether =Adam= falls under =torch.optim.Optimizer= when running code
********* rename unsemantic functions such as =read_docs= to =read_doc(ument)=
********* look into ISO hard encoding when reading files -> perhaps this can be modified
******** documentation
********* provide only description of data structures (eg. data, labels) required for training processes -> remove actua data
********* consider changing default helpers in readme to python helpers instead of those from shell scripts

****** run SoPa++ for multiple runs to survey performance -> run on all variants and data-set portions with (repeated) grid-search to get plenty of candidates, means and standard deviations
       DEADLINE: <2021-02-01 Mon>
******* if possible: extend workflow to other RASA NLU data sets given time and resources -> would require new pre-processing scripts
****** with decent model performance, branch off to improve explainability with weighting of patterns -> do this and the previous task simultaneously
       DEADLINE: <2021-02-01 Mon>

***** Long-term
****** update metadata in scripts later with new workflows, eg. with help scripts, comments describing functionality and readme descriptions for git hooks
****** add pydocstrings to all functions for improved documentation
****** add MIT license when made public
****** make list of all useful commands for slurm

**** SoPa++
***** extensions
****** leverage dynamic sub-word-level embeddings from recent advancements in Transformer-based language modeling.
****** modify the architecture and hyperparameters to use more wildcards or self-loops, and verify the usefulness of these in the mimic WFSA models.
****** modify the output multi-layer perceptron layer to a general additive layer, such as a linear regression layer, with various basis functions. This would allow for easier interpretation of the importance of patterns without the use of occlusion -> perhaps consider adding soft logic functions which could emulate negation/inclusion of rules, or possibly a soft decision tree at the top layer
****** test SoPa++ on multi-class text classification tasks 
      
**** SoPa
***** goods: practical new architecture which maps to RNN-CNN mix via WFSAs, decent code quality in PyTorch (still functional), contact made with author and could get advice for possible extensions
***** limitations
****** SoPa utilizes static word-level token embeddings which might contribute to less dynamic learning and more overfitting towards particular tokens
****** SoPa encourages minimal learning of wildcards/self-loops and $\epsilon$-transitions, which leads to increased overfitting on rare words such as proper nouns
****** while SoPa provides an interpretable architecture to learn discrete word-level patterns, it is also utilizes occlusion to determine the importance of various patterns. Occlusion is usually a technique reserved for uninterpretable model architectures and contributes little to global explainability
****** SoPa was only tested empirically on binary text classification tasks
***** issues
****** unsure what self-loops and fwd-1s mean in output of =visualize.py= -> GitHub issue made to request for more information: https://github.com/Noahs-ARK/soft_patterns/issues/8 
***** general: likely higher performance due to direct inference and less costly conversion methods

**** Data sets
***** NLU data sets -> single sequence intent classification, typically many classes involved -> eg. ATIS, Snips, AskUbuntuCorpus, FB task oriented dataset (mostly intent classifications)
***** SOTA scores for NLU can be found on https://github.com/nghuyong/rasa-nlu-benchmark#result
***** vary training data sizes from 10% to 70% for perspective on data settings

**** Constraints
***** work with RNNs only
***** seq2cls tasks -> eg. NLU/NLI/semantic tasks, try to work with simpler single (vs. double) sequence classification task
***** base main ideas off peer-reviewed articles 

**** Research questions
***** To what extent does SoPa++ contribute to competitive performance on NLU tasks?
***** To what extent does SoPa++ contribute to improved explainability by simplification?
***** What interesting and relevant explanations does SoPa++ provide on NLU task(s)?

*** Admin
**** Timeline
***** +Initial thesis document: *15.09.2020*+
***** +Topic proposal draft: *06.11.2020*+
***** +Topic proposal final: *15.11.2020*+
***** Topic registration: *01.02.2021* 
***** Manuscript submission: *18.03.2021* 

**** Manuscript notes
***** Text-related feedback
****** 20-90 pages thesis length -> try to keep ideas well-motivated yet succinct
****** make abstract more specific in terms of "highly performant"
****** sub-word embeddings are both useful for performance and explainability
****** fix absolute terms such as "automated reasoning", or quote directly from paper
****** re-consider reference to Transformers for dynamic sub-word level word-embeddings
****** improve capitalization with braces in bibtex file
***** Concept-related feedback
****** clarify meaning and concept of "occlusion" as leave-one-out perturbation analysis
****** improve arbitrary vs. contrained oracle phrasing -> perhaps black-box vs. white-box but more specific
****** expound on trade-off between performance and explainability and process of mimic extraction
****** add more information on what competitive performance means (eg. within few F_1 points)
****** how to evaluate improved explainability -> make hierarchy for local vs. global explainability -> also explainability is only relevant if the oracle and mimic models both *perform competitively and have similar confusion matrix profiles* (both conditions must be satisfied)
****** further work: porting this technique to a transformer where possible
***** Self-thoughts
****** semirings, abstract algebra and how they are used for finite-state machines in Forward and Viterbi algorithms -> go deeper into this to get some background
****** Chomsky hierarchy of languages -> might be relevant especially relating to CFGs
****** FSA/WFSAs -> input theoretical CS, mathematics background to describe these
****** ANN's historical literature -> describe how ANNs approximate symbolic representations
****** extension/recommendations -> transducer for seq2seq tasks
       
** Completed
***** DONE major code refactoring for main model with conversion to recent PyTorch (eg. 1.*) and CUDA versions (eg. 10.*)
      CLOSED: [2020-12-05 Sat 18:47] DEADLINE: <2020-12-06 Sun>
***** DONE add tensorboard to explicit dependencies to view relevant logs during training
      CLOSED: [2020-12-03 Thu 14:40]
***** DONE replace all Variable calls with simple Tensors and add =requires_grad= argument directly to tensors where this is necessary: see https://stackoverflow.com/questions/57580202/whats-the-purpose-of-torch-autograd-variable
      CLOSED: [2020-12-02 Wed 21:50]
***** DONE UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument
      CLOSED: [2020-12-02 Wed 18:57]
***** DONE UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead
      CLOSED: [2020-12-02 Wed 18:39]
***** DONE make workflow to download Facebook Multilingual Task Oriented Dataset and pre-process to sopa-ready format -> text data and labels with dictionary mapping as to what the labels mean
      CLOSED: [2020-12-01 Tue 20:29] DEADLINE: <2020-12-03 Thu>
***** DONE fixed: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead
      CLOSED: [2020-11-30 Mon 18:16]
***** DONE sort CLI arguments into proper groups, sort them alphabetically for easier reading
      CLOSED: [2020-11-30 Mon 18:07]
***** DONE add types to =parser_utils.py= script internals
      CLOSED: [2020-11-30 Mon 18:07]
***** DONE separate extras in =soft_patterns.py= into =utils.py= -> test out how batch is utilized -> fix batch issue, then move on to other steps -> batch mini-vocab appears to be a hack to create a meta-vocabulary for indices -> try to push with this again another time -> consider reverting Vocab index/token defaults in case this was wrong
      CLOSED: [2020-11-30 Mon 18:07]
***** DONE appears to be major bug in Batch class, try to verify if it is indeed a bug and how it can be fixed
      CLOSED: [2020-11-30 Mon 18:07]
***** DONE extract all arg parser chunks and place in dedicated file
      CLOSED: [2020-11-30 Mon 18:07]
***** DONE clean preprocessing script for GloVe vectors and understand inner mechanisms
      CLOSED: [2020-11-28 Sat 17:02]
***** DONE find better location to place code from =util.py=
      CLOSED: [2020-11-27 Fri 19:38]
***** DONE migrate to soft-patterns-pp and clean from there
      CLOSED: [2020-11-26 Thu 20:11]
***** DONE update proposal with comments from supervisors -> update same information here
      CLOSED: [2020-11-17 Tue 14:52] DEADLINE: <2020-11-17 Tue>
***** DONE write proposal with key research questions -> address points directly from step 3 document requirements -> prepare some basic accuracy metrics and interpretations from best model   
      CLOSED: [2020-11-10 Tue 18:45] DEADLINE: <2020-11-06 Fri>
***** DONE analyze pattern log more closely with code on the side to understand what it means -> can start writing early when things start to make sense
      CLOSED: [2020-11-10 Tue 18:44] DEADLINE: <2020-11-05 Thu>
***** DONE add large amounts of binary data for testing with CPU/GPU -> requires pre-processing
      CLOSED: [2020-11-10 Tue 18:21]
***** DONE find re-usable code for running grid search -> otherwise construct makeshift quick code
      CLOSED: [2020-11-05 Thu 20:38]
***** DONE test SoPa on sample data in repository to ensure it works out-of-the-box -> try this on laptop and s3it 
      CLOSED: [2020-11-02 Mon 16:40]
***** DONE make workflow to reproduce virtual environment cleanly via poetry
      CLOSED: [2020-11-02 Mon 16:34]
***** DONE make workflow to download simple but high-quality NLU dataset and glove data sets
      CLOSED: [2020-11-01 Sun 20:15] DEADLINE: <2020-11-01 Sun>
***** DONE read more into these tasks and find one that has potential for interpretability -> likely reduce task to binary case for easier processing (eg. entailment)
      CLOSED: [2020-10-28 Wed 15:32] DEADLINE: <2020-10-28 Wed>
***** DONE search for popular NLI datasets which have existing RNN models as (almost) SOTAs, possibly use ones that were already tested for eg. RTC or ones used in papers that may have semantic element
      CLOSED: [2020-10-26 Mon 17:57] DEADLINE: <2020-10-28 Wed>
***** DONE explore below frameworks (by preference) and find most feasible one
      CLOSED: [2020-10-26 Mon 14:28] DEADLINE: <2020-10-26 Mon>
***** DONE add org-mode hook to remove startup visibility headers in org-mode to markdown conversion
      CLOSED: [2020-10-22 Thu 13:28]
***** DONE Set up repo, manuscript and develop log
      CLOSED: [2020-10-22 Thu 12:36]
      
** Legacy
*** Interpretable RNN architectures
**** State-regularized-RNNs (SR-RNNs)
***** good: very powerful and easily interpretable architecture with extensions to NLP and CV
***** good: simple code which can probably be ported to PyTorch relatively quickly
***** good: contact made with author and could get advice for possible extensions
***** problematic: code is outdated and written in Theano, TensorFlow version likely to be out by end of year
***** problematic: DFA extraction from SR-RNNs is clear, but DPDA extraction/visualization from SR-LSTMs is not clear probably because of no analog for discrete stack symbols from continuous cell (memory) states
***** possible extensions: port state-regularized RNNs to PyTorch (might be simple since code-base is generally simple), final conversion to REs for interpretability, global explainability for natural language, adding different loss to ensure words cluster to same centroid as much as possible -> or construct large automata, perhaps pursue sentiment analysis from SR-RNNs perspective instead and derive DFAs to model these
**** Rational recurences (RRNNs)
***** good: code quality in PyTorch, succinct and short
***** good: heavy mathematical background which could lend to more interesting mathematical analyses
***** problematic: seemingly missing interpretability section in paper -> theoretical and mathematical, which is good for understanding
***** problematic: hard to draw exact connection to interpretability, might take too long to understand everything
**** Finite-automation-RNNs (FA-RNNs)
***** source code likely released by November, but still requires initial REs which may not be present -> might not be the best fit
***** FA-RNNs involving REs and substitutions could be useful extensions as finite state transducers for interpretable neural machine translation

*** Interpretable surrogate extraction
***** overall more costly and less chance of high performance       
***** FSA/WFSA extraction
****** spectral learning, clustering
****** less direct interpretability
****** more proof of performance needed -> need to show it is better than simple data learning

*** Neuro-symbolic paradigms
***** research questions
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables      
