** Tasks 
*** Research

**** Data sets
***** TODO search for popular NLU datasets which have existing RNN models as (almost) SOTAs
***** TODO read more into these tasks and find one that has potential for interpretability -> likely reduce task to binary case for easier processing
      
**** Code and documentation
***** TODO start populating repository with hooks, data downloads, documentation and models
***** TODO populate manuscript and repository with key research questions which can be answered

**** Interpretable architectures
***** **Overall:** likely higher performance due to direct inference and less costly
***** TODO explore below frameworks (by preference) and find most feasible one
***** SoPA and rational recurrences
****** first is a practical approach while second is highly theoretical
****** provide interpretable architectures which need to be further explored
****** both implementations have decent code quality
***** State-regularized-RNNs
****** practical and offers direct interpretability from architecture
****** code is outdated and written in Theano, TensorFlow version likely to be out by end of year*
****** possible work: port state-regularized RNNs to PyTorch with CUDA headers
****** final conversion to REs for interpretability
***** Finite-automation-RNNs
****** source code likely released by November, but still requires initial REs which may not be present
****** FA-RNNs involving REs and substitutions could be useful extensions as finite state transducers for interpretable neural machine translation
       
**** Interpretable surrogate extraction
***** **Overall:** more costly and less chance of high performance       
***** FSA/WFSA extraction
****** spectral learning, clustering
****** less direct interpretability
****** more proof of performance needed -> need to show it is better than simple data learning

**** Constraints
***** work with RNNs only
***** seq2cls tasks -> eg. NLU/semantic, paraphrase detection
***** base main ideas off peer-reviewed articles 

**** High-level
***** **globally explainable** -> exposes inner mechanisms and global biases which could help for ethical and adversarial problem detections
***** **high-performance** -> competitive with similar non-explainable learnint techniques
***** think about what this research adds that is not present -> possible to look at next steps in existing articles or possibly extension to new sequence classification tasks
***** develop feasible and interesting research questions, writing will be easy when everything else is well defined

*** Admin    
***** keep good communication with supervisors -> every 3 weeks for Sharid and more regularly with Mathias 
**** General timeline
***** +Initial thesis document: 15.09.20+
***** Topic proposal draft: 06.11.20
***** Topic proposal final: 15.11.20
***** Topic registration: 01.02.20  
***** Manuscript submission: 18.03.20, try to extend if possible  

*** Manuscript-extras
***** read more about turing machines and FSA/WFAs to get theoretical background
***** ann's historical literature find all -> especially focusing on how ANNs approximate symbolic representations which would motivate overall topic
***** convergence, universal approximation and generalization are satisfied by ANNs to a high degree, semantic relevance in the final model is not guaranteed and this needs to be an additional task that where symbolic frameworks are needed    
***** limit main experiments on sequence classification but mention transducer extension to seq2seq
***** show that new rnn performs competitively with others on same task but is interpretable  and explainable, show the explainability in best way possible as a slice, emphasize global nature of model
***** if possible, bring in theoretical CS and mathematics into paper
     
** Legacy
*** Neuro-symbolic paradigms
***** research questions:
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions:
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables
