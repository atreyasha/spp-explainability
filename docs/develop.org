#+STARTUP: overview
 
** Tasks
*** Research
**** High-level
***** *globally explainable* -> exposes inner mechanisms and global biases which could help for ethical and adversarial problem detections
***** *high-performance* -> competitive with similar non-explainable learnint techniques
***** *contributions* -> should add insights which are new and not commonly found in research so far

**** Data sets
***** TODO search for popular NLU datasets which have existing RNN models as (almost) SOTAs, possibly use ones that were already tested for eg. RTC or ones used in papers that may have semantic element
      DEADLINE: <2020-10-28 Wed>
***** TODO read more into these tasks and find one that has potential for interpretability -> likely reduce task to binary case for easier processing (eg. entailment)
      DEADLINE: <2020-10-28 Wed>

**** Clean-code and documentation
***** TODO write proposal and manuscript with key research questions which can be answered
      DEADLINE: <2020-11-06 Fri>
***** start populating repository with hooks, data downloads, documentation and models 
      
**** Interpretable neural architectures
***** Soft patterns (SoPa)
****** good: practical new architecture which maps to RNN-CNN mix via WFSAs
****** good: code quality in PyTorch, lengthy code
****** good: contact made with author and could get advice for possible extensions
****** problematic: global explainability might be a far shot, occlusion is still used for documents
****** problematic: not clear how this could be linked to a final WFSA -> perhaps it is ultimately not
****** possible extensions: improve on larger data, learnable word embeddings, sub-word pre-processing to leverage morphology, increase generalization with wildcards, improve interpretability via in-built method instead of occlusion in document analysis setting, final additive layer for finding relevance of patterns
***** State-regularized-RNNs (SR-RNNs) 
****** good: very powerful and easily interpretable architecture with extensions to NLP and CV
****** good: simple code which can probably be ported to PyTorch relatively quickly
****** good: contact made with author and could get advice for possible extensions
****** problematic: code is outdated and written in Theano, TensorFlow version likely to be out by end of year
****** problematic: DFA extraction from SR-RNNs is clear, but DPDA extraction/visualization from SR-LSTMs is not clear probably because of no analog for discrete stack symbols from continuous cell (memory) states
****** possible extensions: port state-regularized RNNs to PyTorch (might be simple since code-base is generally simple), final conversion to REs for interpretability, global explainability for natural language, adding different loss to ensure words cluster to same centroid as much as possible -> or construct large automata, perhaps pursue sentiment analysis from SR-RNNs perspective instead and derive DFAs to model these
***** *GIST:* likely higher performance due to direct inference and less costly

**** Constraints
***** work with RNNs only
***** seq2cls tasks -> eg. NLU/semantic, paraphrase detection
***** base main ideas off peer-reviewed articles 

*** Admin
**** General timeline
***** +Initial thesis document: 15.09.20+
***** Topic proposal draft: 06.11.20
***** Topic proposal final: 15.11.20
***** Topic registration: 01.02.20  
***** Manuscript submission: 18.03.20, try to extend if possible  
***** *Note:* meeting every 3 weeks with Sharid and more regularly with Mathias 

**** Manuscript notes
***** FSA/WFSAs -> input theoretical CS, mathematics background to describe these
***** ann's historical literature -> describe how ANNs approximate symbolic representations
***** extension/recommendations -> transducer for seq2seq tasks

** Completed
***** DONE explore below frameworks (by preference) and find most feasible one
      CLOSED: [2020-10-26 Mon 14:28] DEADLINE: <2020-10-26 Mon>
***** DONE add org-mode hook to remove startup visibility headers in org-mode to markdown conversion
      CLOSED: [2020-10-22 Thu 13:28]
***** DONE Set up repo, manuscript and develop log
      CLOSED: [2020-10-22 Thu 12:36]
      
** Legacy
*** Interpretable RNN architectures
**** Rational recurences (RRNNs)
***** good: code quality in PyTorch, succinct and short
***** good: heavy mathematical background which could lend to more interesting mathematical analyses
***** problematic: seemingly missing interpretability section in paper -> theoretical and mathematical, which is good for understanding
***** problematic: hard to draw exact connection to interpretability, might take too long to understand everything
**** Finite-automation-RNNs (FA-RNNs)
***** source code likely released by November, but still requires initial REs which may not be present -> might not be the best fit
***** FA-RNNs involving REs and substitutions could be useful extensions as finite state transducers for interpretable neural machine translation

*** Interpretable surrogate extraction
***** overall more costly and less chance of high performance       
***** FSA/WFSA extraction
****** spectral learning, clustering
****** less direct interpretability
****** more proof of performance needed -> need to show it is better than simple data learning

*** Neuro-symbolic paradigms
***** research questions:
****** can we train use a neuro-symbolic paradigm to attain high performance (similar to NNs) for NLP task(s)?
****** if so, can this paradigm provide us with greater explainability about the inner workings of the model?

*** Neural decision trees
***** decision trees are the same as logic programs -> the objective should be to learn logic programs
***** hierarchies are constructed in weight-space which lends itself to non-sequential models very well -> but problematic for token-level hierarchies
***** research questions:
****** can we achieve similar high performance using decision tree distillation techniques (by imitating NNs)?
****** can this decision tree improve interpretability/explainability?
****** can this decision tree distillation technique outperform simple decision tree learning from training data?

*** Inductive logic on NLP search spaces
***** can potentially use existing IM models such as paraphrase detector for introspection purposes in thesis
***** n-gram power sets to explore for statistical artefacts -> ANNs can only access the search space of N-gram power sets -> solution to NLP tasks must be a statistical solution within the power sets which links back to symbolism
***** eg. differentiable ILP from DeepMind
***** propositional logic only contains atoms while predicate/first-order logic contain variables
