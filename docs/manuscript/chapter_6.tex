\chapter{Conclusions}

\label{chapter:conclusions}

In this chapter, we summarize the key findings of this thesis. We
started off this thesis by emphasizing the importance of \ac{xai} research and
correspondingly laid out clear definitions of \ac{xai}-related concepts adapted
from \citet{arrieta2020explainable}. Through our own survey of recent literature
on explainability techniques used in \ac{nlp}, we came across several
interesting studies and drew particular inspiration from
\citet{schwartz2018sopa} who developed the novel \ac{sopa} model. While functioning
well, we found \ac{sopa}'s explainability techniques to be localized and indirect
despite its neural architecture being suited for the globalized and direct
explanations by simplification explainability technique. This inspired our main
objective to propose a modified model \ac{sopa}++, which could allow for
effective explanations by simplification. To guide us in addressing our
objective, we listed our three research questions in Section \ref{section:rq}.

The most significant changes from \ac{sopa} to \ac{sopa}++ include the utility of strict
linear-chain \ac{wfaws} over linear-chain \ac{wfas}, replacement of the \ac{mlp}
in \ac{sopa} with quantized and transparent hidden layers and the introduction of a
new globalized and direct explanations by simplification post-hoc explainability
technique to simplify the black-box \ac{sopa}++ model into a transparent \ac{re}
proxy model. With these changes, we proceeded to answer our three research
questions.

Regarding our first research question, we observe that \ac{sopa}++'s best accuracy
range on the \ac{fmtod} data set of 97.6-98.3$\%$ falls into the competitive accuracy
range of 96.6-99-5$\%$ based on other recent studies. In this respect, we
conclude that \ac{sopa}++ offers competitive performance on the \ac{fmtod} English
language intent classification task.

Regarding our second research question, we compare the accuracy scores and
distance metrics between \ac{sopa}++ and \ac{re} proxy model pairs and observe accuracy
differences as low as $\sim$1$\%$ and softmax distance norms as small as $\sim$4$\%$ for
medium and large-sized models with $\tau$-thresholds ranging from 0.50-1.00. We
therefore conclude that the explanations by simplification post-hoc
explainability technique is effective on the \ac{fmtod} English language intent
classification task given larger model sizes and $\tau$-thresholds.

Regarding our third and final research question, we identify salient \ac{tauste}
neurons which received disproportionately large relative linear weights in our
best performing small \ac{re} proxy model. Next, we analyze regular expression
samples in the \ac{re} lookup layer from the aforementioned \ac{re} proxy model
corresponding to these salient neurons. Based on an analysis of these sampled
regular expressions, we observe several interesting phenomena such as lexical
segmentation, branching transitions with tokens having similar lexical semantics
and also the presence of USA-centric inductive biases captured from the training
data.

\clearpage

With these answers to our research questions, we addressed our objective of
proposing a modified \ac{sopa}++ model that could allow for effective
explanations by simplification. However, we did encounter several limitations to
the \ac{sopa}++ model while answering our research questions. In the next
chapter, we expound on these limitations and how they could be addressed in
future research.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 