\chapter{Further work}

\label{chapter:further_work}

In this final chapter, we address the limitations of our thesis and provide
ideas for future research which could address these limitations.

\section{Efficiency}

In this thesis, we showed the effectiveness of simplifying SoPa++ models into RE
proxy models. In the course of our experiments, we realized that while this
process was effective; the simplification and evaluation processes using RE
proxy models tended to be slow. This is largely because we used single-threaded
processes for both simplification and evaluation of RE proxy models; in contrast
to PyTorch's parallelized functionalities on a GPU which made SoPa++ very fast.
To overcome the low efficiency of simplifcation and evaluation using RE proxy
models, we could recommend two approaches. One approach could be to save the RE
lookup layer as a data base and utilize indexed searches to make regular
expression searches and lookups much faster. Another approach could be to
utilize GPU-accelerated regular expression matching algorithms to parallelize
the overall RE lookup layer and its searching functionalities
\citep{wang2011gregex,zu2012gpu,yu2013gpu}.

\section{Explainability}

As mentioned in Section \ref{section:evaluate_explain}, we were only able to
address the technical requirements of explanations by simplification without
necessarily delving into how good the explanations provided are for given target
audiences. This process is subjective and would require a thorough survey with a
given target audience. Conducting such a survey of quality of explanations could
be useful to further evaluate the explainability of SoPa++ and its RE proxy
counterparts.

\section{Discovery and correction of inductive biases}

As shown in Section \ref{section:discussion_regex}, we provided examples of how
a human could manually find and correct inductive biases in the RE lookup layer.
It would be interesting to explore how extensively this process could be
conducted to find and correct inductive biases in the REs inside the RE lookup
layer. Furthermore, it would be interesting if we could also find adversarial
samples in this layer. While correcting the RE lookup layer ``fixes'' problems
on the RE proxy model's side, it does not modify the functionality of the
antecedent SoPa++ model. It would therefore also be interesting to explore how
we could propagate the corrections in the RE lookup layer back into the SoPa++
model's neural network components.

\section{Generalization}

Based on sampled regular expressions shown in Figures
\ref{fig:regex_example_neuron_21}, \ref{fig:regex_example_neuron_27} and
\ref{fig:regex_example_neuron_32}, we can observe certain transitions which
contain many possible words. We can conduct semantic analyses on these highly
utilized transitions and perhaps generalize these to include more tokens which
may even be outside of the initial model vocabulary. For example, in the fourth
RE from the top in Figure \ref{fig:regex_example_neuron_21}, we can observe that
the third transition generally captures numbers formatted as time. We could
generalize this transition to capture any tokens that express the time; which
could lead to generalization on previously unseen data instances. This could
also address issues of unknown tokens altogether. 

\section{Modeling extensions}

Since our current SoPa++ passes binary outputs from the TauSTE layer to the
linear regression layer, we can infer that the classification outputs will be
discretized as are the TauSTE outputs. As a result, we could transform the
linear regression layer during the SoPa++ to RE proxy simplification phase into
a decision tree; which could make the RE proxy model even more transparent given
a small enough decision tree. This process could however be difficult for larger
models with more TauSTE neurons; therefore this process could be more viable for
smaller models.

Another possible extension could be to extend SoPa++'s weighted finite-state
automata to finite-state transducers; which are highly similar but return scored
seqeuences instead of only path scores. In this way, SoPa++ with finite-state
transducers could even be used for sequence-to-sequence tasks; making it viable
to other applications in NLP such as Neural Machine Translation (NMT). Finally,
it would be interesting to see how SoPa++ performs in tasks given longer
sequence lengths since the FMTOD data set generally contains short input
utterances.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 