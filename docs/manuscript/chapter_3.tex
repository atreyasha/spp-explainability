\chapter{Methodologies}

\label{chapter:methodologies}

In this chapter, we describe the methodologies used in this thesis.
Comprehensive source code reflecting these methodologies can be found in our
public GitHub
repository\footnote{https://github.com/atreyasha/spp-explainability}.

\section{Facebook Multilingual Task Oriented Dialog}

\citet{schuster-etal-2019-cross-lingual} originally released the Facebook
Multilingual Task Oriented Dialog (FMTOD) data set to encourage research in
cross-lingual transfer learning for Natural Language Understanding (NLU) tasks;
specifically from from high-resource to low-resource languages. The authors
released the FMTOD data set with English as the high-resource language providing
$\sim$43k annotated utterances, and Spanish and Thai as low-resource languages
providing a total of $\sim$14k utterances. Furthermore, they streamlined the
data set on two key tasks; namely intent detection and textual slot filling. In
this thesis, we focus solely on the English language intent detection task in
the FMTOD data set. This intent detection task entails a multi-label sequence
classification task with a total of 12 classes from alarm, reminder and
weather-related domains.

\subsection{Motivation}

We chose to work with the FMTOD data set since it is both a recently released
and well-studied data set
\citep{schuster-etal-2019-cross-lingual,zhang2019joint,zhang-etal-2020-intent}.
We focus on the English language intent classification task since it is a
relatively straightforward task which allows us to place a greater focus on
performance and explainability. Furthermore, the English language subset entails
the highest resources in the FMTOD data set. Finally, we find the FMTOD data
set's intent detection classification especially attractive because it allows us
to test the SoPa++ model on a multi-class NLU problem; which is significantly
different from the focus on binary classification sentiment detection tasks in
SoPa \citep{schwartz2018sopa}.

\subsection{Preprocessing}

We enumerate our preprocessing steps below:

\begin{enumerate}{}
  \item Similar to \citet{schwartz2018sopa}, we convert all FMTOD text samples
  to a lowercased format. This assists in simplifying the data set further.
  \item Next, we search through the pre-provided training, validation and test
  data partitions to remove duplicates within each partition.
  \item Finally, we remove data duplicates which overlap between partitions.
  During this step, we do not remove any cross-partition duplicates from the
  test partition in order to keep it as similar as possible to the original test
  partition. This comes into importance later when we compare performance
  evaluations on the test set with other studies.
\end{enumerate}

\begin{figure}[t]
  \centering
  \includegraphics[width=14cm]{pdfs/generated/fmtod_summary_statistics.pdf}
  \caption{Data distribution of the preprocessed FMTOD data set grouped by
    classes and partitions}
  \label{fig:fmtod}
\end{figure}

\begin{table}[t!]
  \centering
  \begin{tabular}{lllll}
    \toprule
    Class and description & Train & Validation & Test & $\Sigma$ \\
    \midrule
    0: \texttt{alarm/cancel\_alarm} & 1157 & 190 & 444 & 1791 \\
    1: \texttt{alarm/modify\_alarm} & 393 & 51 & 122 & 566 \\
    2: \texttt{alarm/set\_alarm} & 3584 & 596 & 1236 & 5416 \\
    3: \texttt{alarm/show\_alarms} & 619 & 83 & 212 & 914 \\
    4: \texttt{alarm/snooze\_alarm} & 228 & 49 & 89 & 366 \\
    5: \texttt{alarm/time\_left\_on\_alarm} & 233 & 30 & 81 & 344 \\
    6: \texttt{reminder/cancel\_reminder} & 662 & 114 & 284 & 1060 \\
    7: \texttt{reminder/set\_reminder} & 3681 & 581 & 1287 & 5549 \\
    8: \texttt{reminder/show\_reminders} & 474 & 82 & 217 & 773 \\
    9: \texttt{weather/check\_sunrise} & 63 & 13 & 25 & 101 \\
    10: \texttt{weather/check\_sunset} & 88 & 11 & 37 & 136 \\
    11: \texttt{weather/find} & 9490 & 1462 & 3386 & 14338 \\[5pt]
    \hline \hline \\[-10pt]
    $\Sigma$ & 20672 & 3262 & 7420 & 31354 \\
    \bottomrule
  \end{tabular}
  \caption{Frequency of the preprocessed FMTOD data set classes grouped by
    partitions; $\Sigma$ signifies the cumulative frequency statistic}
  \label{tab:fmtod}
\end{table}

Many of the duplicates observed were already present in the original FMTOD data
set, with additional duplicates being created from the initial lowercasing step.
After preprocessing, we obtain a lowercased variant of the FMTOD data set with
strictly unique data partitions. In the next section, we describe the summary
statistics of the preprocessed FMTOD data set.

\begin{table}[t!]
  \centering
  \begin{threeparttable}
    \begin{tabular}{lll}
      \toprule
      Class and description & Utterance length$^{\dagger}$ & Example$^{\ddagger}$ \\
      \midrule
      0: \texttt{alarm/cancel\_alarm} & 5.6 $\pm$ 1.9 & cancel weekly alarm \\
      1: \texttt{alarm/modify\_alarm} & 7.1 $\pm$ 2.5 & change alarm time \\
      2: \texttt{alarm/set\_alarm} & 7.5 $\pm$ 2.5 & please set the new alarm \\
      3: \texttt{alarm/show\_alarms} & 6.9 $\pm$ 2.2 & check my alarms. \\
      4: \texttt{alarm/snooze\_alarm} & 6.1 $\pm$ 2.1 & pause alarm please \\
      5: \texttt{alarm/time\_left\_on\_alarm} & 8.6 $\pm$ 2.1  & minutes left on my alarm \\
      6: \texttt{reminder/cancel\_reminder} & 6.6 $\pm$ 2.2 & clear all reminders. \\
      7: \texttt{reminder/set\_reminder} & 8.9 $\pm$ 2.5 & birthday reminders \\
      8: \texttt{reminder/show\_reminders} & 6.8 $\pm$ 2.2 & list all reminders \\
      9: \texttt{weather/check\_sunrise} & 6.7 $\pm$ 1.7 & when is sunrise \\
      10: \texttt{weather/check\_sunset} & 6.7 $\pm$ 1.7 & when is dusk \\
      11: \texttt{weather/find} & 7.8 $\pm$ 2.3 & jacket needed? \\[5pt]
      \hline \hline \\[-10pt]
      $\mu$ & 7.7 $\pm$ 2.5 & \textemdash \\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}[flushleft]
      \footnotesize
      \item $^{\dagger}$Summary statistics follow the mean $\pm$
      standard-deviation format
      \item $^{\ddagger}$Short and simple examples were chosen for brevity and
      formatting purposes
    \end{tablenotes}
  \end{threeparttable}
  \caption{Tabular summary of utterance length statistics and examples for FMTOD
    data classes; $\mu$ signifies the cumulative summary statistics}
  \label{tab:fmtod-examples}
\end{table}

\begin{table}[t!]
  \centering \def\arraystretch{1.3}
  \begin{tabular}{L{0.27\linewidth} L{0.45\linewidth} l}
    \toprule
    Study & Summary & Accuracy \\
    \midrule
    \citet{schuster-etal-2019-cross-lingual} & BiLSTM jointly trained on both the slot filling and intent detection English language tasks & 99.1$\%$ \\
    \citet{zhang2019joint} & BERT along with various decoders jointly fine-tuned on both the slot filling and intent detection English language tasks & 96.6--98.9$\%$ \\
    \citet{zhang-etal-2020-intent} & RoBERTa and XLM-RoBERTa fine-tuned on the English language and multilingual intent detection tasks respectively along with WikiHow pre-training & 99.3--99.5$\%$ \\
    \bottomrule
  \end{tabular}
  \caption{Tabular summary of studies that addressed the FMTOD intent detection
    English language task, along with their relevant summaries and accuracy
    range(s)}
  \label{tab:fmtod-results}
\end{table}

\subsection{Summary statistics}

Figure \ref{fig:fmtod} shows the summary statistics of the preprocessed FMTOD
data set grouped by classes and data set partitions. Similarly, Table
\ref{tab:fmtod} shows the same summary statistics in a tabular form with
explicit frequencies. Based on the summary statistics, we can observe that the
preprocessed FMTOD data set is significantly imbalanced with $\sim$45$\%$ of
samples falling into Class 11 alone. We take this observation into consideration
in later sections and apply fixes to mitigate this data imbalance. In addition,
we observe from Table \ref{tab:fmtod-examples} that input utterances in the
preprocessed FMTOD data set are generally short; with a mean input utterance
length of 7.7 and a standard deviation of 2.5 tokens. Utterance length summary
statistics were computed with the assistance of NLTK's default \texttt{Treebank}
word tokenizer \citep{bird-loper-2004-nltk}.

\subsection{Performance range}

Several studies have optimized deep learning models on the FMTOD English
language intent classification task using a variety of models from BiLSTMs to
XLM-RoBERTa
\citep{schuster-etal-2019-cross-lingual,zhang2019joint,zhang-etal-2020-intent}.
Table \ref{tab:fmtod-results} summarizes these studies along with their reported
accuracy scores on the FMTOD English language intent classification task. Based
on the presented results from these recent studies, we can infer that the
general competitive accuracy range for the FMTOD English language intent
classification task is from 96.6$\%$ to 99.5$\%$.

\section{SoPa++}

In this section we describe our SoPa++ model's architecture and present both
similarities and differences compared to the SoPa model in
\citet{schwartz2018sopa}.

\subsection{Tokenization and word embeddings}

Similar to \citet{schwartz2018sopa}, we utilize NLTK's default \texttt{Treebank}
word tokenizer \citep{bird-loper-2004-nltk} to conduct tokenization of input
utterances into word-level tokens. While \citet{schwartz2018sopa} utilize GloVe
840B 300-dimensional true-cased embeddings, we utilize GloVe 6B 300-dimensional
uncased word-level embeddings \citep{pennington2014glove} to project the input
tokens in utterances to continuous numerical spaces. We utilized this smaller
subset of word-level embeddings since it allowed us to work with lower-cased
words, as well as experiment with a variety of embedding dimensions during our
development phase.

\subsection{Weighted finite state automaton-$\omega$}

As mentioned in Section \ref{section:soft-patterns}, \citet{schwartz2018sopa}
constructed the SoPa model with an ensemble of linear-chain WFAs which permitted
both epsilon and self-loop transitions. As noted in Section
\ref{section:sopa-model-specs}, epsilon and self-loop transitions are useful
constructs in abstracting WFAs and allowing them to match variable length
strings. However based on our experimentation during our development phase, we
observed a key concern that the highest scoring paths in the WFAs in SoPa tended
to have a large variation of string lengths due to the effect of both
epsilon-transitions and self-loops. We believe that this reduced the impact of
SoPa's explainability methods and as a result, the first change we decided for
was to remove both epsilon and self-loop transitions. With this change, we could
at least ensure that each WFA would match strings of fixed lengths.

However, matching strings of fixed lengths could also be seen as a form of
overfitting in the model; since a model could simply memorize short strings or
phrases and would not necessarily "learn" to generalize. To address this
concern, we include a wildcard transition which we define here as a
$\omega$-transition. Allowing for such a transition was only natural since
wildcards are already crucial parts of regular expressions; which as we
mentioned are equivalent to FAs. To support this, we provide the following
definition for a WFA-$\omega$:

\begin{figure}[t]
  \centering
  \includegraphics[width=14cm]{pdfs/generated/w_nfa_linear_chain/main.pdf}
  \caption{Schematic visualizing a strict linear-chain finite-state automaton with
    a $\omega$-transition extracted from a WFA-$\omega$}
  \label{fig:fsa-w}
\end{figure}

\begin{definition}[Weighted finite-state automaton-$\omega$]
  \label{def:w-wsa}
  A weighted finite-state automaton-$\omega$ over a semiring $\mathbb{K}$ is a
  5-tuple $\mathcal{A} = \langle \Sigma, \mathcal{Q}, \Gamma, \lambda, \rho
  \rangle$, with:

  \begin{itemize}
  \itemsep0em
    \item[--] a finite input alphabet $\Sigma$;
    \item[--] a finite state set $\mathcal{Q}$;
    \item[--] transition weights $\Gamma: \mathcal{Q} \times \mathcal{Q} \times (\Sigma \cup \{\omega\}) \rightarrow \mathbb{K}$;
    \item[--] initial weights $\lambda: \mathcal{Q} \rightarrow \mathbb{K}$;
    \item[--] and final weights $\rho: \mathcal{Q} \rightarrow \mathbb{K}$.
  \end{itemize}

  \begin{remark}
    An $\omega$ transition is equivalent to a wildcard transition, which
    consumes an arbitrary token input and moves to the next state
  \end{remark}

  \begin{remark}
    Besides the inclusion of the $\omega$-transition and removal of the
    $\epsilon$-transition, an $\omega$-WFA has all of the same characteristics
    as the WFA defined in Definition \ref{def:wfa}.
  \end{remark}
\end{definition}

With the removal of epsilon and self-loop transitions and the introduction of
the WFA-$\omega$, our SoPa++ model now uses strict linear-chain WFA-$\omega$'s
instead of the linear-chain WFAs in SoPa. These changes allow us to attain fixed
string length matches with an added layer of generalization because of the
introduction of wildcards. An example of a strict-linear chain FA extracted from
a WFA-$\omega$ is shown in Figure \ref{fig:fsa-w}. Interestingly, we can observe
that this FA corresponds to the Perl-compatible regular expression \texttt{``what a
  (great|entertaining) [\^{}\textbackslash\textbackslash s]+ !''}, where
\texttt{[\^{}\textbackslash\textbackslash s]+} refers to any set of consecutive
characters which are not separated by a space character; which we interpret as
a wildcard or $\omega$-transition.

\subsection{Tau straight-through estimator}

In Section \ref{section:ste} we described the concept of a STE in activation
quantized neural networks and explained how STEs function in both their forward
and backward passes. Furthermore, we provided a motivation as to why STEs and
other quantized activation functions are of interest; for example in relation to
computational savings linked to low-precision computing. In our implementation
of the SoPa++ model, we make use of a variant of the STE activation function;
which we define here as the Tau straight-through estimator (TauSTE).

\begin{equation}
  \label{eq:tau-ste-forward}
  \text{TauSTE}(x)=
  \begin{cases}
    1 & x \in (\tau, +\infty) \\
    0 & x \in (-\infty, \tau]
  \end{cases}
\end{equation}

\begin{equation}
  \label{eq:tau-ste-backward}
  \text{TauSTE}'(x)=
  \begin{cases}
    1 & x \in  (1, +\infty) \\
    x & x \in [-1, 1] \\
    -1 & x \in (-\infty, -1) \\
  \end{cases}
\end{equation}

\begin{figure}[t!]
  \centering
  \includegraphics[width=14cm]{pdfs/generated/tau_ste_applied/main.pdf}
  \caption{Schematic visualizing the TauSTE's forward and backward passes}
  \label{fig:tau-ste}
\end{figure}

Figure \ref{fig:tau-ste} shows a schematic of the TauSTE's forward and backward
passes. As we can see, there are two key changes from the vanilla STE to the
TauSTE. Firstly, the threshold for activation in the forward function is now
governed by some variable $\tau \in \mathbb{R}$. This was done to allow for some degree of freedom
in deciding the activation threshold. Secondly, the backward pass returns the
identity function on a limited range on inputs; specifically on $x \in [-1,1]$.
This restriction on the backward pass was placed to ensure that gradients do not
blow up in size.

As we describe in detail in the next sections, we chose to use this special
variant of the STE because we believe that the TauSTE could prove to be useful
for the explainability purposes of our SoPa++ model. This is mainly because the
TauSTE (or even the STE) activation function simplifies continuous inputs into
discrete outputs; thereby reducing the information content of the signal it
receives as inputs. In later sections, we show how we utilize and capitalize
this reduction in signal information in our SoPa++ model.

\begin{figure}[t!]
  \centering
  \includegraphics[width=15cm]{pdfs/generated/spp_computational_graph/main.pdf}
  \caption{Schematic visualizing the computational graph of the SoPa++ model}
  \label{fig:cg-spp}
\end{figure}

\subsection{Forward pass}

In this subsection, we describe the forward pass of the SoPa++ model by
specifically referring to its several neural components. This description is
linked to the visualization of the computational graph of SoPa++ in Figure
\ref{fig:cg-spp} and SoPa++ forward pass algorithm in \ref{algo:spp-forward}.
Similar to \citet{schwartz2018sopa}, we firstly process input utterances by
padding their start and end indices with special \texttt{[START]} and
\texttt{[END]} tokens respectively. Next, we project input tokens to numerical
spaces using the aforementioned GloVe word embeddings followed by an affine
transformation to achieve the appropriate dimensionality for the following
steps. Following this, we use our emsemble of $m \in \mathbb{N}$ pre-defined
WFA-$\omega$'s to traverse the input text by scoring consecutive substrings
present in the input text. When processing each input token, we monitor the end
states of each of the $m \in \mathbb{N}$ WFA-$\omega$ and max-pool the score
present in this state; which is analogous to the Viterbi algorithm
\citep{viterbi1967error}. In the edge case that an input string was too short
for the end state of WFA-$\omega$ to register a score, we simply discard this
score in further analysis. This description so far corresponds to the lower half
of Figure \ref{fig:cg-spp}.

After max-pooling scores from all the $m \in \mathbb{N}$ WFA-$\omega$'s, we
then pass this collection of scores for further processing using SoPa++'s other
neural components. This can be seen in the upper portion of Figure
\ref{fig:cg-spp}. Firstly, we apply layer normalization \citep{ba2016layer} to
all pattern or WFA-$\omega$ scores without applying any additional affine
transformations. We omit the affine transformation to not alter any of the
pattern score information content and use layer normalization as an expedient
means of projecting the values of pattern scores to a standard normal
distribution. This furthermore guarantees that pattern scores would be
smaller in size and would have a roughly even distribution of small positive and
negative values around 0.

This projection to a standard normal distribution becomes very useful as we next
encounter the TauSTE layer. Here, this layer maps all inputs which are strictly
larger than the $\tau \in \mathbb{R}$ threshsold to 1 and all others inputs to
zero. The TauSTE layer naturally is only useful when it is able to discriminate
the inputs by mapping some of them to 1 and some to 0, instead of always mapping
all inputs to either 1 or 0. Without layer normalization, the TauSTE layer would
not be able to perform its function since pattern scores tend to be mostly
positive with differing ranges. Layer normalization therefore helps to project
these variations of scores to a uniform range; which ultimately allows the
TauSTE layer to be useful.

A natural criticism of the TauSTE layer could be that it strongly limits the
flow of information in the SoPa++ model. While the binarization present in this
layer does indeed limit the rich flow of continuous numerical information, it is
still worth noting that this layer can preserve sufficient information given a
sufficiently large number of WFA-$\omega$ and therefore TauSTE neurons. For
example, if we allow for $m=40$ and therefore 40 WFA-$\omega$ and TauSTE
neurons, we can technically have a total to 2$^{40}\approx1.1\times10^{12}$
binary state possibilities; which is roughly equal to the estimated number of stars
present in the Andromeda Galaxy \citep{10.1093/mnras/stu879}. This would imply
that despite the reduction in information content on the TauSTE layer, there are
still sufficient mappable states available to learn various representations.

\begin{algorithm}[t!]
  \small
  \caption{SoPa++ forward pass}
  \label{algo:spp-forward}
  \begin{algorithmic}[1]
    \Require{Word-tokenized input utterance and initialized SoPa++ model}
    \Ensure{SoPa++ classification label}
    \Statex
    \Begin
    \State $utterance \gets pad(utterance)$ \Comment{Pad with [START] and [END] tokens}
    \State $utterance \gets glove(utterance)$ \Comment{Projection to GloVe word
      embeddings}
    \ForEach{WFA-$\omega_i \in$ [WFA-$\omega_1,\ldots,$WFA-$\omega_m$]} 
    \Comment{Loop over all $m$ WFA-$\omega$}
    \State $\text{WFA-}\omega_i \gets viterbi(utterance, \text{WFA-}\omega_i)$
    \Comment{Traverse utterance using Viterbi}
    \State $s_i \gets final\_state(\text{WFA-}\omega_i)$ \Comment{Extract final
      state score of WFA-$\omega_i$}
    \EndFor
    \State $S \gets [s_i,\ldots,s_m]$ \Comment{Gather all pattern scores and
      assemble as vector}
    \State $S \gets layer\_norm(S)$ \Comment{Layer normalization of all
      pattern scores}
    \State $S \gets tau\_ste(S)$ \Comment{Apply TauSTE on all normalized values} 
    \State $S \gets linear(S)$ \Comment{Linear transformation to class
    dimensionality}
    \State $S \gets softmax(S)$ \Comment{Softmax to project to probability space}
    \State $L \gets argmax(S)$ \Comment{Argmax to retrieve the highest scoring class}
    \State \Return $L$
    \End
  \end{algorithmic}
\end{algorithm}

After binarizing the input values in the TauSTE layer, we apply a simple linear
or affine transformation to the inputs to modify their dimensionality from $m
\in \mathbb{N}$ to $n \in \mathbb{N}$, where the $n$ represents the number of
output classes. We specifically chose a linear regression layer over a MLP
because linear regressors are known to be transparent models
\citep{arrieta2020explainable} and this is a feature which ultimately assists us
in the explainability portion of the SoPa++ model, which we describe in greater
detail in the next sections.

Finally, after applying the linear transformation; we simply apply a softmax
function over the linear outputs and extract the highest scoring index to
represent the predicted class. In the case of Figure \ref{fig:cg-spp}, the
output class for the input pre-processed sentence \texttt{``[START] 10 day
  forecast [END]''} is the \texttt{weather/find} label which corresponds to class 11.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 